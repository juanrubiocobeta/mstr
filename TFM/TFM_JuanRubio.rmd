---
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: pdflatex
    keep_tex: false
bibliography: bibliografia.bib
link-citations: yes 
geometry: "left=3cm,right=2.5cm,top=2.5cm,bottom=2.5cm"
fontsize: 11pt
lang: es-ES
header-includes:
  - \usepackage{setspace}
  - \setstretch{1.2}
  - \usepackage{indentfirst}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{titlesec}
  - \titleformat{\chapter}[display]{\normalfont\huge\bfseries\centering}{\chaptertitlename\ \thechapter}{15pt}{\Huge}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhead{}
  - \fancyhead[RO,LE]{\thepage}
  - \fancyfoot{}
  - \usepackage{hyperref}
  - \hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=blue}
  - \usepackage{verbatim}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  
  size = "scriptsize",
  collapse = TRUE,
  comment = "#>",
  strip.white = TRUE,
  
  fig.align = "center",
  out.width = "70%",
  fig.pos = "H"
)
```

\begin{titlepage}

    \vspace{2cm}

    \centering
    {\Large Máster en Estadística Aplicada}\\[0.5cm]
    {\large Departamento de Estadística e Investigación Operativa}\\[0.5cm]
    {\large Universidad de Granada}
    
    \vspace{2cm}
    
    \includegraphics[width=0.5\textwidth]{ugr.png} 
    
    \vspace{2cm}
    
    {\large Trabajo fin de máster}\\[1cm]
    
    {\large Estimación KDE: fundamentos y aplicaciones}
    
    \vspace{2cm}
    
    {\large Juan Rubio Cobeta}\\[0.5cm]
    
    {\large Granada, \today}
    
\end{titlepage}

\newpage

\thispagestyle{empty}

\begin{titlepage}
    \centering

    \vspace{0.5cm}

    {\large Máster en Estadística Aplicada}\\[0.5cm]

    {\large Departamento de Estadística e Investigación Operativa}\\[0.5cm]

    {\large Universidad de Granada}

    \vspace{2cm}

    \includegraphics[width=0.4\textwidth]{ugr.png} 

    \vspace{2cm}

    \begin{center}
    \textbf{Trabajo de investigación presentado por Don Juan Rubio y dirigido por la profesora Dña. Maria Dolores Martínez Miranda.}
    \end{center}

    \vspace{2cm}

    \begin{center}
        VºBº
    \end{center}

    \vspace{2cm}

    \begin{minipage}{0.45\textwidth}
        \centering
        \vspace{0.5cm}
        \textbf{Maria Dolores Martínez Miranda}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \vspace{0.5cm}
        \textbf{Juan Rubio Cobeta}
    \end{minipage}
\end{titlepage}

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic} 


# Introducción y Motivación

## Contexto y Justificación

La inferencia estadística constituye el pilar fundamental para la extracción de conocimiento a partir de datos observados. En el contexto del Máster en Estadística Aplicada de la Universidad de Granada, este trabajo aborda uno de los problemas centrales de la disciplina: la estimación de la función de densidad de probabilidad subyacente a una variable aleatoria, sin imponer restricciones fuertes sobre su forma funcional.

Tradicionalmente, la estadística clásica ha abordado este problema desde un enfoque **paramétrico**. Bajo este paradigma, se asume que los datos provienen de una familia conocida de distribuciones $\mathcal{P} = \{f(x; \theta) : \theta \in \Theta\}$, como la distribución Normal, Gamma o Weibull. En tal escenario, el problema de estimación se reduce drásticamente a la inferencia de un vector de parámetros finito-dimensional $\theta$. Si bien este enfoque es potente y eficiente cuando el modelo asumido es correcto, presenta una debilidad crítica: la **rigidez**. La realidad de los fenómenos estocásticos complejos raramente se ajusta con precisión a estas formas ideales. La imposición de una estructura paramétrica incorrecta conlleva un *sesgo de especificación* sistemático que puede invalidar cualquier análisis posterior, ocultando características locales importantes.

Como respuesta a esta limitación, surge la **estadística no paramétrica** y, específicamente, los métodos de suavizado. La filosofía subyacente es permitir que los datos "hablen por sí mismos" (*data-driven approach*), dejando que sea la propia estructura de la muestra la que determine la forma de la densidad estimada $\hat{f}$.

Históricamente, el **histograma** ha sido la herramienta precursora en esta tarea. A pesar de su ubicuidad, el histograma padece de defectos teóricos y prácticos notables:

1.  Es una función discontinua, lo que impide el uso de herramientas de cálculo diferencial.

2.  Su forma depende excesivamente de la elección del origen de la partición y del ancho de los intervalos.

3.  Carece de propiedades de eficiencia en dimensiones altas.

Para solventar estas deficiencias, @rosenblatt1956 y @parzen1962 formalizaron el **Estimador de Densidad Tipo Núcleo** (Kernel Density Estimator o KDE). Este estimador generaliza la idea del histograma colocando una "masa de probabilidad" suave (el núcleo) sobre cada observación, proporcionando una estimación diferenciable e integrable a uno.

## Motivación Práctica

La relevancia de estudiar el estimador KDE trasciende el interés puramente teórico; su aplicación es crítica en escenarios donde detectar desviaciones sutiles respecto a la normalidad es vital para la gestión de riesgos. Este trabajo toma como caso de estudio central el índice **Standard & Poor's 500 (S&P 500)**, considerado el barómetro más representativo del mercado bursátil global.

Uno de los "hechos estilizados" más documentados en la econometría financiera es que la distribución de los rendimientos de los activos no sigue una distribución normal: presenta colas más pesadas (*leptocurtosis*) y asimetrías que los modelos gaussianos subestiman sistemáticamente. Ignorar estas características, especialmente durante periodos de crisis financiera, lleva a una infravaloración del riesgo de eventos extremos (cisnes negros).

El uso de la Estimación de Densidad Tipo Núcleo permite capturar la forma real de la distribución de los rendimientos del S&P 500 sin supuestos previos. Esto es fundamental para:

1.  **Cálculo de métricas de riesgo:** Obtener estimaciones más precisas del Valor en Riesgo (VaR) y del *Expected Shortfall* en las colas de la distribución.

2.  **Detección de regímenes de mercado:** Visualizar cómo la densidad de los rendimientos cambia de forma bimodal o se aplana en periodos de alta volatilidad, comportamientos que un modelo paramétrico estático no podría revelar.

La aplicación práctica de este trabajo ([Capítulo 7](#capitulo7)) demostrará cómo el KDE ofrece una radiografía superior del comportamiento del mercado en comparación con el ajuste de una curva de campana tradicional.

## Objetivos y Estructura del Trabajo

El objetivo principal de este Trabajo Fin de Máster es presentar una revisión teórica rigurosa, exhaustiva y autocontenida de la estimación de densidad tipo núcleo, analizando sus propiedades asintóticas, los desafíos de la selección del ancho de banda y su aplicación a datos financieros reales.

Para garantizar una exposición clara y detallada, la memoria se estructura en los siguientes capítulos:

* **Capítulo 2:** Se establecen los **preliminares probabilísticos**, fijando la notación y recordando definiciones clave sobre convergencia estocástica y funciones de distribución.
* **Capítulo 3:** Se dedica a la construcción formal del **Estimador Núcleo Univariante**, definiendo los requisitos analíticos de la función núcleo $K$ y presentando las familias de núcleos más usuales.
* **Capítulo 4:** Aborda las **propiedades estadísticas** del estimador. Se descompone el error de estimación (MISE) en sesgo y varianza, demostrando el compromiso (*trade-off*) necesario.
* **Capítulo 5:** Trata el problema central de la práctica del KDE: la **selección del ancho de banda** ($h$). Se discuten métodos como la Regla de Silverman y la Validación Cruzada (LSCV).
* **Capítulo 6:** Extiende los conceptos al caso **multivariante**.
* **Capítulo 7:** Presenta la **aplicación práctica** al índice S&P 500. Se realizará un análisis exploratorio y se aplicarán los estimadores desarrollados teóricamente para modelar la distribución de sus rendimientos, comparando los resultados obtenidos mediante diferentes selecciones de ancho de banda.
* Finalmente, se presentan las **conclusiones** y líneas de trabajo futuro.

\newpage 

# Preliminares Probabilísticos

El objetivo de este capítulo es establecer el marco matemático y la notación que se utilizarán a lo largo del trabajo. Dado que la estimación de densidad tipo núcleo se fundamenta en propiedades de convergencia de sumas de variables aleatorias, es necesario definir con rigor los conceptos de espacio de probabilidad, variable aleatoria y los modos de convergencia estocástica.

## Fundamentos de Probabilidad

Comenzamos definiendo la estructura formal donde se modelan los experimentos aleatorios.

**Definición 2.1 (Espacio de Probabilidad).**

Un espacio de probabilidad es una terna $(\Omega, \mathcal{F}, \mathbb{P})$, donde:

1.  $\Omega$ es el **espacio muestral**, un conjunto no vacío que contiene todos los posibles resultados del experimento.

2.  $\mathcal{F}$ es una **$\sigma$-álgebra** sobre $\Omega$, es decir, una colección de subconjuntos de $\Omega$ que cumple:

    * $\Omega \in \mathcal{F}$.

    * Si $A \in \mathcal{F}$, entonces su complemento $A^c \in \mathcal{F}$.

    * Si $\{A_n\}_{n=1}^{\infty} \subset \mathcal{F}$, entonces $\bigcup_{n=1}^{\infty} A_n \in \mathcal{F}$.

3.  $\mathbb{P}: \mathcal{F} \to [0, 1]$ es una **medida de probabilidad**, que satisface $\mathbb{P}(\Omega) = 1$ y la propiedad de $\sigma$-aditividad para conjuntos disjuntos.

En el contexto de la inferencia estadística, trabajamos con funciones que trasladan la aleatoriedad desde el espacio abstracto $\Omega$ a los números reales.

**Definición 2.2 (Variable Aleatoria Real).**

Dado un espacio de probabilidad $(\Omega, \mathcal{F}, \mathbb{P})$, una función $X: \Omega \to \mathbb{R}$ se dice que es una **variable aleatoria** (v.a.) si es $\mathcal{F}$-medible; es decir, si para todo conjunto de Borel $B \in \mathcal{B}(\mathbb{R})$ (la $\sigma$-álgebra generada por los intervalos abiertos de $\mathbb{R}$), se cumple que la preimagen $X^{-1}(B) \in \mathcal{F}$.

Cada variable aleatoria induce una medida de probabilidad sobre $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$, caracterizada unívocamente por su función de distribución acumulada.

**Definición 2.3 (Función de Distribución).**

La función de distribución (CDF, por sus siglas en inglés) de una variable aleatoria $X$ se define como la función $F: \mathbb{R} \to [0, 1]$ tal que:
$$F(x) = \mathbb{P}(X \le x) = \mathbb{P}(\{\omega \in \Omega : X(\omega) \le x\}), \quad \forall x \in \mathbb{R}$$

Esta función es no decreciente, continua por la derecha y satisface $\lim_{x \to -\infty} F(x) = 0$ y $\lim_{x \to \infty} F(x) = 1$.

En este trabajo nos centraremos exclusivamente en variables aleatorias **absolutamente continuas**. Formalmente, esto significa que la medida inducida por $X$ es absolutamente continua respecto a la medida de Lebesgue $\mu$ en $\mathbb{R}$. Por el Teorema de Radon-Nikodym [@billingsley1995], esto garantiza la existencia de una función de densidad.

**Definición 2.4 (Función de Densidad de Probabilidad).**

Se dice que una variable aleatoria $X$ tiene una función de densidad de probabilidad (PDF) $f$, si existe una función no negativa e integrable tal que para todo conjunto de Borel $B \in \mathcal{B}(\mathbb{R})$:
$$\mathbb{P}(X \in B) = \int_{B} f(t) \, dt$$

Como consecuencia inmediata, la relación entre la función de distribución y la densidad viene dada por:
$$F(x) = \int_{-\infty}^{x} f(t) \, dt$$

Además, si $f$ es continua en $x$, entonces por el Teorema Fundamental del Cálculo se cumple que:
$$f(x) = F'(x) = \frac{d}{dx} F(x)$$

El objetivo central de la estimación de densidad no paramétrica es estimar esta función $f$ desconocida a partir de una muestra aleatoria simple $X_1, \dots, X_n$ distribuida según $F$.

## Momentos

Una vez definida la densidad de probabilidad, las características numéricas más relevantes de una variable aleatoria son sus momentos. Estos valores resumen la información contenida en la densidad, describiendo aspectos como la localización, la dispersión o la forma de la distribución. En el contexto de la estimación tipo núcleo, el cálculo de esperanzas y varianzas de transformaciones de la variable aleatoria constituirá la herramienta analítica principal para evaluar la calidad del estimador $\hat{f}$.

**Definición 2.5 (Esperanza Matemática).**

Sea $X$ una variable aleatoria absolutamente continua con función de densidad $f$. Se define la **esperanza matemática** (o valor esperado) de $X$, denotada por $\mathbb{E}[X]$ o $\mu$, como la integral de Lebesgue:
$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x) \, dx$$
siempre que la integral sea absolutamente convergente, es decir, $\int_{-\infty}^{\infty} |x| f(x) \, dx < \infty$. Si esta condición no se cumple, la esperanza no existe.

La esperanza es un operador lineal. Dadas dos variables aleatorias $X, Y$ definidas sobre el mismo espacio de probabilidad y constantes $a, b \in \mathbb{R}$, se satisface:
$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$

Para el desarrollo de las propiedades del estimador núcleo, raramente trabajaremos directamente con $\mathbb{E}[X]$, sino con la esperanza de funciones de $X$ (específicamente, el núcleo $K$ evaluado en los datos). El siguiente resultado permite calcular dicha esperanza utilizando directamente la densidad original $f$.

**Teorema 2.1 (Esperanza de una función de una v.a.).**

Sea $g: \mathbb{R} \to \mathbb{R}$ una función medible Borel. Si $X$ es una variable aleatoria con densidad $f$, entonces:
$$\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) \, dx$$
siempre que $\mathbb{E}[|g(X)|] < \infty$.

Este resultado es crucial en este trabajo. Por ejemplo, al calcular el sesgo del estimador, evaluaremos la esperanza de $g(X_i) = \frac{1}{h}K\left(\frac{x-X_i}{h}\right)$, lo que se traducirá directamente en una integral de convolución entre el núcleo y la densidad verdadera.

**Definición 2.6 (Varianza y Momentos Superiores).**

La **varianza** de una variable aleatoria $X$, denotada por $\mathbb{V}(X)$ o $\sigma^2$, es una medida de dispersión cuadrática en torno a la media. Se define como el segundo momento central:
$$\mathbb{V}(X) = \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right] = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) \, dx$$

Desarrollando el cuadrado, se obtiene la expresión operativa habitual:
$$\mathbb{V}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

De forma general, definimos el **$k$-ésimo momento** respecto al origen como $\mu'_k = \mathbb{E}[X^k]$, y el **$k$-ésimo momento central** como $\mu_k = \mathbb{E}[(X-\mu)^k]$. La varianza corresponde a $\mu_2$. La existencia de estos momentos depende del decaimiento de las colas de la densidad $f$; distribuciones de cola pesada (como las observadas en finanzas) pueden carecer de momentos de orden superior.

**Propiedades de la Varianza.**

1.  **No negatividad:** $\mathbb{V}(X) \ge 0$.

2.  **Invarianza por traslación y escala:** Para constantes $a, b \in \mathbb{R}$, $\mathbb{V}(aX + b) = a^2 \mathbb{V}(X)$.

3.  **Aditividad:** Si $X_1, \dots, X_n$ son variables aleatorias independientes dos a dos, entonces la varianza de la suma es la suma de las varianzas:
    $$\mathbb{V}\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \mathbb{V}(X_i)$$
Esta última propiedad es fundamental para derivar la varianza del estimador KDE, ya que este se construye como una suma lineal de variables aleatorias independientes.

**Teorema 2.2 (Cambio de Variable en Integración).**

Dado que el análisis asintótico del ancho de banda $h$ implica reescalar la variable aleatoria, recordamos la fórmula de cambio de variable, esencial para las demostraciones del Capítulo 4. Sea $\phi: \mathbb{R} \to \mathbb{R}$ un difeomorfismo. Entonces:
$$\int_{\mathbb{R}} f(\phi(u)) |\phi'(u)| \, du = \int_{\mathbb{R}} f(x) \, dx$$

En particular, para la transformación lineal $x = \mu + hu$ (con $h > 0$), tenemos $dx = h \, du$. Esto nos permite reescribir integrales de convolución de la forma:
$$\int_{-\infty}^{\infty} \frac{1}{h} K\left(\frac{x - y}{h}\right) f(y) \, dy = \int_{-\infty}^{\infty} K(u) f(x - hu) \, du$$
Esta identidad convierte el parámetro de escala $h$ en un argumento de la función $f$, permitiendo posteriormente aplicar desarrollos de Taylor para analizar el comportamiento local del estimador cuando $h \to 0$.


## Convergencia Estocástica y Notación de Orden

El comportamiento del estimador de densidad tipo núcleo depende intrínsecamente del tamaño de la muestra $n$. Para analizar sus propiedades asintóticas, es decir, cuando $n \to \infty$, es imprescindible formalizar en qué sentido las aproximaciones estocásticas se acercan a los valores teóricos poblacionales.

Sea $\{X_n\}_{n=1}^{\infty}$ una sucesión de variables aleatorias y $X$ una variable aleatoria, todas ellas definidas sobre un mismo espacio de probabilidad $(\Omega, \mathcal{F}, \mathbb{P})$.

### Modos de Convergencia

Distinguimos tres modos principales de convergencia, ordenados de mayor a menor exigencia sobre la estructura probabilística subyacente.

**Definición 2.7 (Convergencia Casi Segura).**

Se dice que la sucesión $\{X_n\}$ converge **casi seguramente** (o con probabilidad 1) a $X$, denotado por $X_n \xrightarrow{c.s.} X$, si el conjunto de eventos donde la sucesión no converge tiene medida nula:
$$\mathbb{P}\left( \left\{ \omega \in \Omega : \lim_{n \to \infty} X_n(\omega) = X(\omega) \right\} \right) = 1$$
Esta es la forma más fuerte de convergencia clásica. En el contexto de estimación, si un estimador $\hat{\theta}_n$ satisface $\hat{\theta}_n \xrightarrow{c.s.} \theta$, se dice que es **fuertemente consistente**.

**Definición 2.8 (Convergencia en Probabilidad).**

La sucesión $\{X_n\}$ converge **en probabilidad** a $X$, denotado por $X_n \xrightarrow{p} X$, si para todo $\epsilon > 0$, la probabilidad de que la distancia entre $X_n$ y $X$ exceda $\epsilon$ tiende a cero:
$$\lim_{n \to \infty} \mathbb{P}(|X_n - X| > \epsilon) = 0$$
Esta convergencia implica la **consistencia débil** de un estimador. Es una condición más relajada que la casi segura, permitiendo que existan eventos raros donde la discrepancia sea grande, siempre que su probabilidad se desvanezca asintóticamente.

**Definición 2.9 (Convergencia en Distribución).**

Sean $F_n$ y $F$ las funciones de distribución acumuladas de $X_n$ y $X$ respectivamente. Se dice que $X_n$ converge **en distribución** (o en ley) a $X$, denotado por $X_n \xrightarrow{d} X$, si:
$$\lim_{n \to \infty} F_n(x) = F(x)$$
para todo punto $x \in \mathbb{R}$ donde $F$ sea continua.

A diferencia de las anteriores, la convergencia en distribución no requiere que las variables estén definidas en el mismo espacio de probabilidad, ya que se refiere únicamente a sus leyes. Es fundamental para la construcción de intervalos de confianza asintóticos mediante el Teorema Central del Límite.

**Teorema 2.3 (Jerarquía y Relaciones).**

Las implicaciones entre los modos de convergencia son las siguientes:
$$X_n \xrightarrow{c.s.} X \implies X_n \xrightarrow{p} X \implies X_n \xrightarrow{d} X$$
Las implicaciones inversas no son ciertas en general, salvo en el caso particular en que $X$ es una constante degenerada ($X = c$), donde $X_n \xrightarrow{d} c \implies X_n \xrightarrow{p} c$.

Para operar algebraicamente con límites estocásticos, enunciamos dos resultados clásicos que utilizaremos para simplificar las expresiones del error asintótico.

**Teorema 2.4 (Teorema de la Aplicación Continua y Slutsky).**

Sea $g: \mathbb{R} \to \mathbb{R}$ una función continua.

1.  Si $X_n \xrightarrow{p} X$, entonces $g(X_n) \xrightarrow{p} g(X)$.

2.  Si $X_n \xrightarrow{d} X$, entonces $g(X_n) \xrightarrow{d} g(X)$.

3.  **(Lema de Slutsky):** Si $X_n \xrightarrow{d} X$ e $Y_n \xrightarrow{p} c$, donde $c$ es una constante, entonces:
    $$X_n + Y_n \xrightarrow{d} X + c, \quad X_n Y_n \xrightarrow{d} cX, \quad \text{y} \quad X_n / Y_n \xrightarrow{d} X/c \; (\text{si } c \neq 0)$$

### Notación de Orden Estocástica ($o_p$ y $O_p$)

En el análisis asintótico del KDE, descompondremos el error en términos principales y términos residuales que se desvanecen rápidamente. Para formalizar la "velocidad" a la que estos términos convergen, generalizamos la notación de Landau ($o$ y $O$) al contexto probabilístico [@garcia2025].
Sea $\{X_n\}$ una sucesión de variables aleatorias y $\{a_n\}$ una sucesión de constantes positivas reales.

**Definición 2.10 (Pequeña "o" en probabilidad).**

Escribimos $X_n = o_p(a_n)$ si la razón $X_n / a_n$ converge a cero en probabilidad:
$$X_n = o_p(a_n) \iff \frac{X_n}{a_n} \xrightarrow{p} 0$$
Intuitivamente, esto significa que $X_n$ es de un orden asintótico estrictamente menor que $a_n$. Un caso particular importante es $X_n = o_p(1)$, que es equivalente a decir que $X_n \xrightarrow{p} 0$.

**Definición 2.11 (Grande "O" en probabilidad).**

Escribimos $X_n = O_p(a_n)$ si la razón $X_n / a_n$ está **acotada en probabilidad**. Formalmente, esto significa que para todo $\epsilon > 0$, existe una constante finita $M_\epsilon > 0$ y un entero $N_\epsilon$ tales que:
$$\mathbb{P}\left( \left| \frac{X_n}{a_n} \right| > M_\epsilon \right) < \epsilon, \quad \forall n \ge N_\epsilon$$
Si $X_n = O_p(1)$, decimos que la sucesión es **estocásticamente acotada** [@garcia2025]. Cabe destacar que toda sucesión convergente en distribución es $O_p(1)$ (Teorema de Prohorov [@billingsley1995]), lo que implica que $X_n \xrightarrow{d} X \implies X_n = O_p(1)$.

**Álgebra de órdenes estocásticos.**

Estas notaciones simplifican enormemente las demostraciones al permitirnos absorber términos despreciables. Algunas reglas aritméticas que utilizaremos (análogas al caso determinista) son:

1.  $o_p(1) + o_p(1) = o_p(1)$
2.  $O_p(1) + o_p(1) = O_p(1)$
3.  $O_p(1) \cdot o_p(1) = o_p(1)$
4.  Si $X_n = O_p(n^{-\alpha})$ y $Y_n = O_p(n^{-\beta})$, entonces $X_n Y_n = O_p(n^{-(\alpha+\beta)})$.

## Herramientas de Análisis y Aproximación

El análisis de las propiedades del estimador núcleo se basa fundamentalmente en aproximaciones locales de la función de densidad desconocida $f$. Para garantizar la validez de estas aproximaciones, es necesario restringir el espacio de funciones admisibles y disponer de herramientas de cálculo para cuantificar el error de aproximación.

### Clases de Suavidad

La velocidad de convergencia del estimador KDE depende directamente de la "suavidad" de la densidad subyacente. Formalizamos este concepto mediante las clases de diferenciabilidad.

**Definición 2.12 (Espacio $\mathcal{C}^k$).**

Sea $I \subseteq \mathbb{R}$ un intervalo abierto. Definimos $\mathcal{C}^k(I)$ como el conjunto de funciones $g: I \to \mathbb{R}$ que tienen $k$ derivadas continuas en $I$.
En particular, asumiremos habitualmente que la densidad $f \in \mathcal{C}^2(\mathbb{R})$ y que su segunda derivada $f''$ es acotada y continuo-cuadrada integrable. Esta asunción de regularidad es necesaria para asegurar que la curvatura de la densidad está bien definida en todo punto.

### Desarrollo de Taylor

La herramienta principal para evaluar el sesgo del estimador será la expansión de Taylor. Dado que el estimador núcleo evalúa la densidad en puntos vecinos $x - hu$, necesitaremos aproximar $f(x - hu)$ en términos de $f(x)$ cuando el ancho de banda $h$ tiende a cero.

**Teorema 2.5 (Teorema de Taylor con resto de Peano).**

Sea $g \in \mathcal{C}^k(\mathbb{R})$ definida en un entorno del punto $x$. Para un incremento $t$ suficientemente pequeño, se cumple:
$$g(x + t) = g(x) + g'(x)t + \frac{g''(x)}{2!}t^2 + \dots + \frac{g^{(k)}(x)}{k!}t^k + o(|t|^k)$$
donde el término residual $o(|t|^k)$ satisface $\lim_{t \to 0} \frac{o(|t|^k)}{|t|^k} = 0$.

En el contexto del KDE, aplicaremos este teorema con $t = -hu$. Si $f \in \mathcal{C}^2$, la expansión hasta segundo orden resulta:
$$f(x - hu) = f(x) - h u f'(x) + \frac{1}{2} h^2 u^2 f''(x) + o(h^2)$$
Esta expresión es la llave maestra que nos permitirá demostrar en el Capítulo 4 que el sesgo del estimador es de orden cuadrático ($O(h^2)$) respecto al ancho de banda.

### Convolución

Finalmente, observamos que el estimador de densidad puede interpretarse matemáticamente como una operación de convolución.

**Definición 2.13 (Convolución).**

Dadas dos funciones integrables $f, g: \mathbb{R} \to \mathbb{R}$, se define su **convolución**, denotada por $(f * g)(x)$, como:
$$(f * g)(x) = \int_{-\infty}^{\infty} f(y) g(x - y) \, dy$$

\newpage

# El Estimador de Densidad Tipo Núcleo

## Motivación: Del Histograma al *Naive Estimator*

La construcción del estimador de densidad tipo núcleo surge como una evolución natural necesaria para superar las deficiencias analíticas del histograma clásico. Si bien el histograma es la herramienta exploratoria más universal, su formulación matemática presenta limitaciones que lo inhabilitan para la inferencia avanzada.

Consideremos una muestra aleatoria $X_1, \dots, X_n$ de una variable con densidad $f$. El histograma se construye definiendo una partición del soporte en intervalos (*bins*) $B_j$ de longitud $h$, y contando la frecuencia de observaciones en cada uno. Formalmente, para un $x \in B_j$, el estimador histograma es:

$$\hat{f}_H(x) = \frac{1}{nh} \sum_{i=1}^n \mathbb{I}(X_i \in B_j)$$

Este estimador presenta dos problemas fundamentales:

1.  **Discontinuidad:** $\hat{f}_H$ es una función escalonada, con discontinuidades en los bordes de los intervalos. Esto hace imposible obtener derivadas de la densidad estimada, impidiendo el análisis de puntos de inflexión o la curvatura.
2.  **Dependencia de la Rejilla:** La forma del histograma cambia drásticamente dependiendo de la elección del punto de origen de la partición $x_0$. Dos estadísticos con los mismos datos podrían llegar a conclusiones visuales distintas simplemente por empezar los intervalos en puntos diferentes.

Para eliminar la arbitrariedad de la rejilla, @rosenblatt1956 propuso liberar a los intervalos de su posición fija. La idea intuitiva es: en lugar de forzar a $x$ a caer en una caja predefinida, construyamos una caja de ancho $2h$ centrada exactamente en cada $x$.

Matemáticamente, esto se justifica recordando la definición de densidad como la derivada de la función de distribución $F$:
$$f(x) = F'(x) = \lim_{h \to 0} \frac{F(x+h) - F(x-h)}{2h}$$

Si sustituimos la distribución teórica $F$ por la **Función de Distribución Empírica** $F_n(x) = \frac{1}{n}\sum_{i=1}^n \mathbb{I}(X_i \le x)$, obtenemos el llamado *Naive Estimator*:

$$\hat{f}_{Naive}(x) = \frac{F_n(x+h) - F_n(x-h)}{2h} = \frac{1}{2nh} \# \{ X_i : x-h < X_i \le x+h \}$$

Podemos reescribir este estimador introduciendo una función de peso o "función núcleo" auxiliar $w(u)$:
$$\hat{f}_{Naive}(x) = \frac{1}{nh} \sum_{i=1}^n w\left( \frac{x - X_i}{h} \right), \quad \text{donde } w(u) = \frac{1}{2}\mathbb{I}(|u| \le 1)$$

El Naive Estimator resuelve el problema de la dependencia del origen, pero **no resuelve el problema de la suavidad**. Dado que la función $w(u)$ es rectangular (vale 1/2 dentro del intervalo y 0 fuera), el estimador resultante $\hat{f}_{Naive}$ sigue siendo una función escalonada, presentando saltos discontinuos cada vez que una observación entra o sale de la ventana deslizante $(x-h, x+h)$.

Para obtener una estimación que sea suave y diferenciable (clase $\mathcal{C}^\infty$), es necesario reemplazar la función rectangular $w(\cdot)$ por una función suave $K(\cdot)$ que decaiga gradualmente. Esta generalización da lugar al Estimador de Densidad Tipo Núcleo.

### Ilustración Gráfica
A continuación, ilustramos visualmente esta transición. La Figura 3.1 muestra cómo el histograma depende de los intervalos fijos, mientras que el naive estimator centra las cajas en los datos pero mantiene la rugosidad. Finalmente, el estimador núcleo (con núcleo Gaussiano) recupera la suavidad de la densidad subyacente.
```{r comparacion-estimadores, fig.cap="(a) Histograma, (b) Naive estimator (Rectangular), (c) Gaussian Kernel.", fig.align='center', out.width="100%", fig.height=3.5, fig.width=9}
par(mfrow=c(1, 3), mar=c(4, 4, 3, 1)); set.seed(123); n <- 100
datos <- c(rnorm(n*0.6, 2, 1), rnorm(n*0.4, 6, 1.5)); lx <- c(-2, 11); ly <- c(0, 0.25)

hist(datos, breaks=seq(-4, 12, 1), prob=TRUE, main="(a) Histograma", xlab="x", 
col="gray90", border="gray40", xlim=lx, ylim=ly)

plot(density(datos, kernel="rectangular", bw=0.6), main="(b) Naive estimator", 
xlab="x", ylab="Densidad", col="blue", lwd=2, xlim=lx, ylim=ly, zero.line=FALSE)
polygon(density(datos, kernel="rectangular", bw=0.6), col=rgb(0,0,1,0.1), border="blue")

plot(density(datos, kernel="gaussian", bw=0.6), main="(c) KDE (Gaussiano)",
xlab="x", ylab="Densidad", col="darkgreen", lwd=2, xlim=lx, ylim=ly, zero.line=FALSE)
polygon(density(datos, kernel="gaussian", bw=0.6), col=rgb(0,0.5,0,0.1), border="darkgreen")

par(mfrow=c(1, 1))
```

## Definición Formal y Propiedades del Núcleo

Tras establecer la motivación intuitiva, procedemos a la construcción rigurosa del estimador. La generalización del histograma mediante el uso de funciones de ponderación suaves fue formalizada independientemente por @rosenblatt1956 y @parzen1962, dando lugar al objeto central de este estudio: El Estimador de Rosenblatt-Parzen.

**Definición 3.2 (Estimador de Densidad Tipo Núcleo Univariante).**
Sea $X_1, X_2, \dots, X_n$ una muestra aleatoria simple (i.i.d.) proveniente de una población con función de densidad de probabilidad desconocida $f$. Se define el estimador de densidad tipo núcleo (KDE), denotado por $\hat{f}_h(x)$, como:

$$\hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left( \frac{x - X_i}{h} \right), \quad \forall x \in \mathbb{R}$$

donde:

* $K: \mathbb{R} \to \mathbb{R}$ es la función **núcleo** (*kernel*).
* $h > 0$ es el **ancho de banda** (*bandwidth*) o parámetro de suavizado.

Matemáticamente, este estimador puede interpretarse como la **convolución** de la función de densidad empírica con el núcleo reescalado. Si definimos la densidad empírica como una suma de deltas de Dirac, $f_n(x) = \frac{1}{n}\sum \delta(x - X_i)$, y el núcleo reescalado como $K_h(u) = \frac{1}{h}K(u/h)$, entonces:
$$\hat{f}_h(x) = (f_n * K_h)(x) = \int_{-\infty}^{\infty} K_h(x - t) \, dF_n(t)$$
Esta interpretación es fundamental en análisis funcional, pues transfiere las propiedades de regularidad de $K$ a la estimación $\hat{f}_h$.

### Clase de Funciones Admisibles

Para que $\hat{f}_h(x)$ sea un estimador válido de una densidad de probabilidad, la función núcleo $K$ debe satisfacer ciertas restricciones analíticas. Definimos la clase de funciones admisibles $\mathcal{K}$ mediante las siguientes condiciones de regularidad.

**Condición 1: Normalización e Integrabilidad.**

Para garantizar que $\int_{-\infty}^{\infty} \hat{f}_h(x) \, dx = 1$, el núcleo debe ser una densidad de probabilidad:
$$\int_{-\infty}^{\infty} K(u) \, du = 1$$
*Demostración:*
$$
\int \hat{f}_h(x) dx = \frac{1}{n} \sum_{i=1}^n \int \frac{1}{h} K\left(\frac{x-X_i}{h}\right) dx 
$$
Haciendo el cambio de variable $u = (x-X_i)/h$, tenemos $dx = h du$, por lo que la integral vale $\int K(u) du = 1$ para cada sumando.

**Condición 2: No negatividad.**

Para que $\hat{f}_h(x) \ge 0$ para todo $x$, se requiere:
$$K(u) \ge 0, \quad \forall u \in \mathbb{R}$$
Aunque existen núcleos de orden superior que permiten valores negativos para reducir el sesgo asintótico (kernels de corrección de sesgo), estos generan estimaciones que no son densidades válidas. En este trabajo, nos restringiremos a núcleos no negativos (Clase $\mathcal{K}_2$).

**Condición 3: Simetría y Centrado.**

Se asume que $K$ es una función par:
$$K(u) = K(-u)$$
Esto implica que el primer momento del núcleo es nulo (siempre que la integral exista):
$$\mu_1(K) = \int_{-\infty}^{\infty} u K(u) \, du = 0$$
Esta propiedad es crucial para asegurar que la estimación no esté sistemáticamente desplazada hacia la izquierda o derecha de los datos (ausencia de sesgo de fase).

**Condición 4: Momentos Finitos.**

Se requiere que el núcleo tenga varianza finita y no nula. Definimos el segundo momento del núcleo como:
$$\mu_2(K) = \int_{-\infty}^{\infty} u^2 K(u) \, du = \sigma_K^2 > 0$$
Este parámetro $\mu_2(K)$ aparecerá directamente en el término principal del sesgo asintótico. Un núcleo más disperso (mayor $\mu_2(K)$) tenderá a suavizar más la muestra.

**Condición 5: Rugosidad (Norma $L_2$).**

Para el análisis de la varianza del estimador, se requiere que el núcleo sea cuadrado-integrable. Definimos la **rugosidad** del núcleo como:
$$R(K) = \int_{-\infty}^{\infty} K(u)^2 \, du = \|K\|_2^2 < \infty$$
Esta cantidad mide la concentración de la masa del núcleo. Cuanto mayor es $R(K)$, más "picuda" es la función $K$.

### Herencia de Propiedades de Suavidad

Una de las ventajas teóricas más potentes del estimador núcleo frente al histograma es la transmisión de propiedades analíticas.

**Teorema 3.1 (Diferenciabilidad).**
Sea $K \in \mathcal{C}^k(\mathbb{R})$, es decir, $K$ es $k$-veces diferenciable con continuidad. Entonces, el estimador $\hat{f}_h(x)$ también pertenece a $\mathcal{C}^k(\mathbb{R})$ y su $r$-ésima derivada viene dada por:
$$\hat{f}_h^{(r)}(x) = \frac{1}{nh^{r+1}} \sum_{i=1}^{n} K^{(r)}\left( \frac{x - X_i}{h} \right)$$

Este resultado implica que si elegimos un núcleo como el Gaussiano ($K \in \mathcal{C}^\infty$), la densidad estimada será una función infinitamente diferenciable, permitiendo no solo estimar la densidad, sino también sus derivadas (gradientes, curvaturas) sin esfuerzo adicional. Por el contrario, un núcleo como el Triangular ($K \in \mathcal{C}^0$) producirá una estimación continua pero no diferenciable en los puntos de la muestra.

## Tipos de Funciones Núcleo

Aunque el estimador KDE permite utilizar cualquier función que satisfaga las condiciones de la clase $\mathcal{K}$, en la práctica la elección se restringe a un conjunto de funciones paramétricas estándar. La elección del núcleo determina principalmente las propiedades de diferenciabilidad de la estimación $\hat{f}_h$, teniendo un impacto secundario en la eficiencia estadística comparado con la selección del ancho de banda.

Las formas funcionales de los núcleos más habituales se presentan a continuación. Salvo el núcleo Gaussiano, todos ellos tienen soporte compacto en $[-1, 1]$.

\begin{align*}
\text{Gaussiano:} & \quad K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2} \\
\text{Epanechnikov:} & \quad K(u) = \frac{3}{4}(1 - u^2) \cdot \mathbb{I}(|u| \le 1) \\
\text{Biweight (Cuártico):} & \quad K(u) = \frac{15}{16}(1 - u^2)^2 \cdot \mathbb{I}(|u| \le 1) \\
\text{Rectangular:} & \quad K(u) = \frac{1}{2} \cdot \mathbb{I}(|u| \le 1) \\
\text{Triangular:} & \quad K(u) = (1 - |u|) \cdot \mathbb{I}(|u| \le 1) \\
\text{Coseno:} & \quad K(u) = \frac{\pi}{4} \cos\left(\frac{\pi}{2}u\right) \cdot \mathbb{I}(|u| \le 1)
\end{align*}

La Figura 3.3 ilustra la diversidad de formas. Mientras que el núcleo **Gaussiano** y el **Biweight** ofrecen perfiles muy suaves (ideales para visualizar densidades continuas), el **Rectangular** y el **Triangular** presentan discontinuidades en la función o en su derivada, siendo menos deseables para aplicaciones que requieran cálculo de gradientes.

```{r zoologico-nucleos, fig.cap="Catálogo de núcleos. Fila superior: Núcleos suaves (Gaussiano, Epanechnikov, Biweight). Fila inferior: Núcleos con menor regularidad (Rectangular, Triangular, Coseno).", fig.align='center', fig.height=5, fig.width=8}
par(mfrow=c(2, 3), mar=c(3, 3, 2, 1))
u <- seq(-1.5, 1.5, length=5000) 

plot_k <- function(x, y, tit, col) {
  plot(x, y, type="l", lwd=2, col=col, main=tit, ylab="", xlab="", ylim=c(0, 1), xaxt="n")
  polygon(c(min(x), x, max(x)), c(0, y, 0), col=adjustcolor(col, 0.2), border=NA)
  axis(1, at=c(-1, 0, 1), labels=c("-1", "0", "1"), cex.axis=0.8); grid()
}

y_g <- dnorm(u); plot_k(u, y_g, "Gaussiano", "blue")
y_e <- ifelse(abs(u)<=1, 0.75*(1-u^2), 0); plot_k(u, y_e, "Epanechnikov", "red")
y_b <- ifelse(abs(u)<=1, (15/16)*(1-u^2)^2, 0); plot_k(u, y_b, "Biweight", "darkorange")
y_r <- ifelse(abs(u)<=1, 0.5, 0); plot_k(u, y_r, "Rectangular", "darkgreen")
y_t <- ifelse(abs(u)<=1, 1-abs(u), 0); plot_k(u, y_t, "Triangular", "purple")
y_c <- ifelse(abs(u)<=1, (pi/4)*cos(pi*u/2), 0); plot_k(u, y_c, "Coseno", "brown")

par(mfrow=c(1, 1))
```

## El Papel del Ancho de Banda ($h$)

El ancho de banda $h$ es el parámetro libre más crítico en la estimación de densidad tipo núcleo. Mientras que la elección del núcleo $K$ determina propiedades cualitativas como la diferenciabilidad de la curva estimada, el parámetro $h$ gobierna las propiedades cuantitativas y estocásticas del estimador: el sesgo y la varianza.

Matemáticamente, el ancho de banda actúa como un factor de escala que determina el tamaño del entorno local sobre el cual se promedian las observaciones. Para comprender su impacto, es necesario formalizar el error que cometemos al estimar $f$.

### Formulación Matemática del Compromiso Sesgo-Varianza

Consideremos el **Error Cuadrático Medio** (MSE) del estimador en un punto $x$ fijo, definido como la esperanza del cuadrado de la diferencia entre el estimador y el valor real:

$$MSE[\hat{f}_h(x)] = \mathbb{E}\left[ (\hat{f}_h(x) - f(x))^2 \right]$$

Mediante la descomposición estándar del error cuadrático, podemos separar este término en dos componentes aditivos: el cuadrado del sesgo y la varianza.

$$MSE[\hat{f}_h(x)] = \underbrace{\left( \mathbb{E}[\hat{f}_h(x)] - f(x) \right)^2}_{\text{Sesgo}^2(x)} + \underbrace{\mathbb{V}[\hat{f}_h(x)]}_{\text{Varianza}(x)}$$

El comportamiento de estos dos componentes respecto a $h$ es antagónico, lo que da lugar al problema fundamental de selección del ancho de banda:

1.  **El Sesgo ($Bias$):** Es una función creciente de $h$. Un ancho de banda grande "aplana" la estimación, alejando sistemáticamente el valor esperado $\mathbb{E}[\hat{f}_h(x)]$ de la verdadera curvatura de la densidad $f(x)$.
    $$\lim_{h \to \infty} \text{Sesgo} \text{ (Alto)}$$
    
2.  **La Varianza ($\mathbb{V}$):** Es una función decreciente de $h$. Un ancho de banda pequeño reduce el número efectivo de observaciones que contribuyen a la estimación en $x$, aumentando la volatilidad muestral.
    $$\lim_{h \to 0} \text{Varianza} \text{ (Alta)}$$

Dado que nuestro objetivo es minimizar el error global sobre todo el dominio, definimos el **Error Cuadrático Integrado Medio (MISE)** como la integral del MSE:

$$MISE(h) = \int_{-\infty}^{\infty} MSE[\hat{f}_h(x)] \, dx = \int_{-\infty}^{\infty} \text{Sesgo}^2(x, h) \, dx + \int_{-\infty}^{\infty} \text{Varianza}(x, h) \, dx$$

El objetivo central de la teoría de selección de ancho de banda, que desarrollaremos analíticamente en los Capítulos 4 y 5, es encontrar el valor $h_{opt}$ que minimice este funcional:

$$h_{opt} = \underset{h > 0}{\text{argmin }} MISE(h)$$

### Ilustración del Fenómeno

Empíricamente, este equilibrio matemático se manifiesta visualmente en la suavidad de la curva. La elección incorrecta de $h$ conduce a dos patologías opuestas:

* **Infrasuavizado (*Undersmoothing*):** Ocurre cuando $h \ll h_{opt}$. La varianza domina el error. La estimación es ruidosa y presenta falsos picos locales generados por la aleatoriedad de la muestra y no por la estructura de la población.
* **Sobresuavizado (*Oversmoothing*):** Ocurre cuando $h \gg h_{opt}$. El sesgo domina el error. La estimación es excesivamente plana, ocultando características importantes como la multimodalidad o la asimetría.

La Figura 3.4 ilustra estos regímenes utilizando el núcleo Gaussiano.

```{r efecto-bandwidth, fig.cap="Ilustración del Bias-Variance Tradeoff.", fig.align='center', out.width="100%", fig.height=4, fig.width=10}
set.seed(123); n <- 100; yl <- c(0, 0.35)
datos <- c(rnorm(n*0.6, 2, 1), rnorm(n*0.4, 6, 1.5))
par(mfrow=c(1, 3), mar=c(4, 4, 3, 1))

d1 <- density(datos, bw=0.15, kernel="gaussian")
plot(d1, main=expression(paste("(a) Infrasuavizado (", h %->% 0, ")")), 
     xlab="x", ylab="Densidad", col="gray40", lwd=1, ylim=yl, zero.line=FALSE)
polygon(d1, col=rgb(0,0,0,0.1), border="gray40"); rug(datos, col="black")

d2 <- density(datos, bw=0.6, kernel="gaussian")
plot(d2, main="(b) Balanceado", xlab="x", ylab="Densidad", col="blue", 
     lwd=2, ylim=yl, zero.line=FALSE)
polygon(d2, col=rgb(0,0,1,0.1), border="blue"); rug(datos, col="blue")

d3 <- density(datos, bw=1.5, kernel="gaussian")
plot(d3, main=expression(paste("(c) Sobresuavizado (", h %->% infinity, ")")), 
     xlab="x", ylab="Densidad", col="red", lwd=2, ylim=yl, zero.line=FALSE)
polygon(d3, col=rgb(1,0,0,0.1), border="red"); rug(datos, col="red")

par(mfrow=c(1, 1))
```

\newpage

# Propiedades Estadísticas del Estimador
## Esperanza y Sesgo (Bias)
## Varianza del Estimador
## Error Cuadrático Medio (MSE) y Consistencia
## Error Global (MISE y AMISE)

\newpage

# Selección del Ancho de Banda
## El Ancho de Banda Óptimo Teórico
## Reglas de Referencia (Plug-in simples)
## Validación Cruzada (Cross-Validation)

\newpage

# Extensión al Caso Multivariante
## Definición en $\mathbb{R}^d$
## Tipos de Matrices de Anchos de Banda
## La Maldición de la Dimensionalidad

\newpage

# Aplicación Práctica: Análisis del S&P 500 {#capitulo7}
## Descripción de los Datos
## Análisis Exploratorio
## Estimación de la Densidad
## Implicaciones de Riesgo

\newpage

# Conclusiones y Líneas Futuras

\newpage

# Bibliografía